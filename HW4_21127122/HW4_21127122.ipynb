{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "\n",
    "Hồ Thanh Nhân - 21127122\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scipy by the reference from this link: https://scipy.org/install/\n",
    "\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [1]\n",
    "\n",
    "For $N > d_{VC}$, we have the approximation: $m_{\\cal{H}}(N) = N^{d_{VC}} \\Leftrightarrow m_{\\cal{H}}(2N) = (2N)^{d_{VC}}$\n",
    "\n",
    "We also have $d_{VC} = 10$\n",
    "\n",
    "\"$95 \\%$ confidence that your generalization error is at most $0.05$\" means: $\\delta = 1 - 0.95 = 0.05$ and $\\epsilon = 0.05$\n",
    "\n",
    "The VC generalization bound is given by: $\\epsilon \\le \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_{\\cal{H}}(2N)}{\\delta}}}$\n",
    "\n",
    "Now, we will find the value of $N$ that satisfies the above inequality, and $N$ is the base to find the closest numerical approximation of the sample size that the VC generalization bound predicts.\n",
    "\n",
    "Let's manipulate the inquality to an equality yields: $\\epsilon = \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_{\\cal{H}}(2N)}{\\delta}}}$\n",
    "\n",
    "$\\Leftrightarrow \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_{\\cal{H}}(2N)}{\\delta}}} - \\epsilon = 0$\n",
    "\n",
    "Let function $f$: $f(N) = \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_{\\cal{H}}(2N)}{\\delta}}} - \\epsilon = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function f(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(N, d_VC, delta, epsilon):\n",
    "    return math.sqrt(8 / N * math.log(4 / delta * (2 * N)**d_VC)) - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of N is:  452957\n"
     ]
    }
   ],
   "source": [
    "# set the intervals (based on the multiple-choice answers)\n",
    "MIN_N = 380000\n",
    "MAX_N = 500000\n",
    "\n",
    "d_VC = 10\n",
    "delta = 0.05\n",
    "epsilon = 0.05\n",
    "root = optimize.brentq(f, MIN_N, MAX_N, args = (d_VC, delta, epsilon))\n",
    "print('Value of N is: ', round(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, the closest numerical approximation of the sample size is [d] 460,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "\n",
    "We have:\n",
    "\n",
    "- $d_{VC} = 50$\n",
    "\n",
    "- $\\delta = 0.05$\n",
    "\n",
    "- $N = 10,000$\n",
    "\n",
    "- $m_{\\cal{H}}(N) = N^{d_{VC}} \\Leftrightarrow m_{\\cal{H}}(2N) = (2N)^{d_{VC}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_VC = 50\n",
    "delta = 0.05\n",
    "N = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the bound in each answer ($f(N)$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [a] Original VC bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_original_VC(N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt(8 / N * math.log(4 / delta * (2 * N) ** d_VC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 0.632174915200836\n"
     ]
    }
   ],
   "source": [
    "print('f(N) =', f_original_VC(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [b] Rademacher Penalty Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Rademacher_Penalty(N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt(2 / N * math.log(2 * N * (N ** d_VC))) + math.sqrt(2 / N * math.log(1 / delta)) + 1 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 0.3313087859616395\n"
     ]
    }
   ],
   "source": [
    "print('f(N) =', f_Rademacher_Penalty(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [c] Parrondo and Van den Broek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Similar to Problem 1, we will consider equality functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Parrondo_and_Van_den_Broek(epsilon, N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt((2 * epsilon + math.log(6 / delta) + d_VC * math.log(2 * N)) / N) - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 0.22369829368078561\n"
     ]
    }
   ],
   "source": [
    "# set the intervals\n",
    "MIN_EPSILON = 0\n",
    "MAX_EPSILON = 1\n",
    "\n",
    "f_Parrondo_and_Van_den_Broek = optimize.brentq(f_Parrondo_and_Van_den_Broek, MIN_EPSILON, MAX_EPSILON, args = (N))\n",
    "print('f(N) =', f_Parrondo_and_Van_den_Broek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [d] Devroye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Similar to Problem 1, we will consider equality functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Devroye(epsilon, N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt((4 * epsilon * (1 + epsilon) + math.log(4 / delta) + 2 * d_VC * math.log(N)) / (2 * N)) - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 0.21522804980824667\n"
     ]
    }
   ],
   "source": [
    "f_Devroye = optimize.brentq(f_Devroye, MIN_EPSILON, MAX_EPSILON, args = (N))\n",
    "print('f(N) =', f_Devroye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, they are not all equal and the smallest bound for $N = 10,000$ is [d] Devroye: $\\epsilon \\le \\sqrt{\\frac{1}{2N}(4 \\epsilon (1 + \\epsilon) + \\ln{\\frac{4 m_{\\cal{H}}(N^2)}{\\delta}})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "\n",
    "We have:\n",
    "\n",
    "- $d_{VC} = 50$\n",
    "\n",
    "- $\\delta = 0.05$\n",
    "\n",
    "- $N = 5$\n",
    "\n",
    "- $m_{\\cal{H}}(N) = N^{d_{VC}} \\Leftrightarrow m_{\\cal{H}}(2N) = (2N)^{d_{VC}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the bound in each answer ($f(N)$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [a] Original VC bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 13.828161484991483\n"
     ]
    }
   ],
   "source": [
    "print('f(N) =', f_original_VC(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [b] Rademacher Penalty Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 7.048776564183685\n"
     ]
    }
   ],
   "source": [
    "print('f(N) =', f_Rademacher_Penalty(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [c] Parrondo and Van den Broek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Similar to Problem 1, we will consider equality functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Parrondo_and_Van_den_Broek_for_P3(epsilon, N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt((2 * epsilon + math.log(6 / delta) + d_VC * math.log(2 * N)) / N) - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 5.101361981989992\n"
     ]
    }
   ],
   "source": [
    "# set the intervals\n",
    "MAX_EPSILON = 20\n",
    "\n",
    "f_Parrondo_and_Van_den_Broek = optimize.brentq(f_Parrondo_and_Van_den_Broek_for_P3, MIN_EPSILON, MAX_EPSILON, args = (N))\n",
    "print('f(N) =', f_Parrondo_and_Van_den_Broek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [d] Devroye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Similar to Problem 1, we will consider equality functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Devroye_for_P3(epsilon, N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt((4 * epsilon * (1 + epsilon) + math.log(4 / delta) + 2 * d_VC * math.log(N)) / (2 * N)) - epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 5.593125543182669\n"
     ]
    }
   ],
   "source": [
    "f_Devroye = optimize.brentq(f_Devroye_for_P3, MIN_EPSILON, MAX_EPSILON, args = (N))\n",
    "print('f(N) =', f_Devroye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, they are not all equal and the smallest bound for $N = 5$ is [c] Parrondo and Van den Broek: $\\epsilon \\le \\sqrt{\\frac{1}{N}(2 \\epsilon + \\ln{\\frac{6 m_{\\cal{H}}(2N)}{\\delta}})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a technique which is called Ordinary Least Squares (OLS).\n",
    "\n",
    "OLS is a type of learning algorithm that can produce a hypothesis (model) that minimizes the mean squared error (MSE) on the examples (training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "The learning model consists of all hypotheses of the form: $h(x) = ax$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize constant\n",
    "NMAX = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_target(x):\n",
    "    return np.sin(np.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_a_hat(N = 2, experiments = NMAX):\n",
    "    total_a = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.array([x_data]).T\n",
    "        \n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        a = weights[0]\n",
    "        \n",
    "        total_a += a\n",
    "\n",
    "    a_hat = total_a / experiments\n",
    "    return round(a_hat, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected value is: g_bar(x) =  1.41 . x\n"
     ]
    }
   ],
   "source": [
    "print('The expected value is: g_bar(x) = ', expected_a_hat(), '. x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the correct answer is **[e] None of the above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    g_bar = expected_a_hat() * x_test\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bias is:  0.3026424541109147\n"
     ]
    }
   ],
   "source": [
    "bias = bias()\n",
    "print('The bias is: ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the closest value to the bias in this case is **[b] 0.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_a_hat = expected_a_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "        x_test = np.random.uniform(-1, 1)\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.array([x_data]).T\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            a = weights[0]\n",
    "\n",
    "            g = a * x_test\n",
    "            g_bar = expected_a_hat * x_test\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance is:  0.2392680784448904\n"
     ]
    }
   ],
   "source": [
    "variance = variance()\n",
    "print('The variance is: ', variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the closest value to the variance in this case is **[a] 0.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\n",
    "$E_{out} = bias + var$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [b] Hypotheses of the form $h(x) = ax$\n",
    "\n",
    "As we analyze in previous Problems, we have the expected value of $E_{out}$ of hypotheses of the form $h(x) = ax$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  0.5419105325558051\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [a] Hypotheses of the form $h(x) = b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_b_hat(N = 2, experiments = NMAX):\n",
    "    total_b = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.ones((N, 1))\n",
    "\n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        b = weights[0]\n",
    "        \n",
    "        total_b += b\n",
    "\n",
    "    b_hat = total_b / experiments\n",
    "    return round(b_hat, 2)\n",
    "\n",
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    g_bar = expected_b_hat()\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset\n",
    "\n",
    "bias = bias()\n",
    "\n",
    "expected_b_hat = expected_b_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.ones((N, 1))\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            b = weights[0]\n",
    "\n",
    "            g = b\n",
    "            g_bar = expected_b_hat\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance\n",
    "\n",
    "variance = variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  0.7649710154522056\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [c] Hypotheses of the form $h(x) = ax + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_weights_hat(N = 2, experiments = NMAX):\n",
    "    total_a = 0\n",
    "    total_b = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.array([x_data, np.ones(N)]).T\n",
    "        \n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        a = weights[0]\n",
    "        b = weights[1]\n",
    "        \n",
    "        total_a += a\n",
    "        total_b += b\n",
    "\n",
    "    a_hat = total_a / experiments\n",
    "    b_hat = total_b / experiments\n",
    "    return round(a_hat, 2), round(b_hat, 2)\n",
    "\n",
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    a_hat, b_hat = expected_weights_hat()\n",
    "    g_bar = a_hat * x_test + b_hat\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset\n",
    "\n",
    "bias = bias()\n",
    "\n",
    "expected_a_hat, expected_b_hat = expected_weights_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "        x_test = np.random.uniform(-1, 1)\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.array([x_data, np.ones(N)]).T\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            a = weights[0]\n",
    "            b = weights[1]\n",
    "\n",
    "            g = a * x_test + b\n",
    "            g_bar = expected_a_hat * x_test + expected_b_hat\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance\n",
    "\n",
    "variance = variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  1.8810110506466935\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [d] Hypotheses of the form $h(x) = ax^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_weights_hat(N = 2, experiments = NMAX):\n",
    "    total_a = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.array([x_data**2]).T\n",
    "        \n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        a = weights[0]\n",
    "        \n",
    "        total_a += a\n",
    "\n",
    "    a_hat = total_a / experiments\n",
    "    return round(a_hat, 2)\n",
    "\n",
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    a_hat = expected_weights_hat()\n",
    "    g_bar = a_hat * x_test**2\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset\n",
    "\n",
    "bias = bias()\n",
    "\n",
    "expected_a_hat = expected_weights_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "        x_test = np.random.uniform(-1, 1)\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.array([x_data**2]).T\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            a = weights[0]\n",
    "\n",
    "            g = a * x_test**2\n",
    "            g_bar = expected_a_hat * x_test**2\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance\n",
    "\n",
    "variance = variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  16.46203867379222\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [e] Hypotheses of the form $h(x) = ax^2 + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_weights_hat(N = 2, experiments = NMAX):\n",
    "    total_a = 0\n",
    "    total_b = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.array([x_data**2, np.ones(N)]).T\n",
    "        \n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        a = weights[0]\n",
    "        b = weights[1]\n",
    "        \n",
    "        total_a += a\n",
    "        total_b += b\n",
    "\n",
    "    a_hat = total_a / experiments\n",
    "    b_hat = total_b / experiments\n",
    "    return round(a_hat, 2), round(b_hat, 2)\n",
    "\n",
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    a_hat, b_hat = expected_weights_hat()\n",
    "    g_bar = a_hat * x_test**2 + b_hat\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset\n",
    "\n",
    "bias = bias()\n",
    "\n",
    "expected_a_hat, expected_b_hat = expected_weights_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "        x_test = np.random.uniform(-1, 1)\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.array([x_data**2, np.ones(N)]).T\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            a = weights[0]\n",
    "            b = weights[1]\n",
    "\n",
    "            g = a * x_test**2 + b\n",
    "            g_bar = expected_a_hat * x_test**2 + expected_b_hat\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance\n",
    "\n",
    "variance = variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  190327.9372684059\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, the learning model has the least expected value of out-of-sample error is **[b] Hypotheses of the form $h(x) = ax$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "\n",
    "We have:\n",
    "\n",
    "$q \\ge 1$ ($q$ is an integer)\n",
    "\n",
    "$m_H(1) = 2$\n",
    "\n",
    "Growth function satisfies: $m_H(N + 1) = 2m_H(N) - {{N} \\choose q}$\n",
    "\n",
    "Recall: ${{M} \\choose m} = 0$ when $m > M$\n",
    "\n",
    "We know that $d_{VC} < break\\ point\\ k$\n",
    "\n",
    "Let's call $d$ is the value of $d_{VC}$\n",
    "\n",
    "It's clear that: $m_H(d) = 2^d$ and $m_H(d - 1) = 2^{d - 1}$\n",
    "\n",
    "Let's analyze the recursive definition for $N = d - 1$, we have:\n",
    "\n",
    "$$m_H(d) = 2m_H(d - 1) - {{d - 1} \\choose q}$$\n",
    "\n",
    "$$\\Leftrightarrow 2^d = 2 \\times 2^{d - 1} - {{d - 1} \\choose q}$$\n",
    "\n",
    "$$\\Leftrightarrow {{d - 1} \\choose q} = 0$$\n",
    "\n",
    "$\\Rightarrow q > d - 1$ (Recall)\n",
    "\n",
    "$\\Leftrightarrow d < q + 1$\n",
    "\n",
    "$\\Leftrightarrow d \\le q$ ($q$ is an integer)\n",
    "\n",
    "\n",
    "This indicates that the VC dimension can take on various values, denoted as $q, q - 1, q - 2, ... 1$. The VC dimension is distinct, and among these potential values, the highest possible value is considered, leading us to the conclusion stated as $q$.\n",
    "\n",
    "Therefore, the answer is **[c] $q$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "\n",
    "$d_{vc}(\\bigcap_{k=1}^{K} H_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Upper bound:\n",
    "\n",
    "Here we have two upper bounds in the answers: $\\sum_{k=1}^{K} d_{VC}(H_k)$ and $K - 1 + \\sum_{k=1}^{K} d_{VC}(H_k)$.\n",
    "\n",
    "Upper bound $\\sum_{k=1}^{K} d_{VC}(H_k)$ shows that the upper limit is the sum of VC dimensions of hypotheses.\n",
    "\n",
    "Now, let's prove it wrong by presenting a counterexample.\n",
    "Suppose we have two hypotheses: **$H_1$** is used to classify all points in the range $[-1,1] \\times [-1,1]$ as +1, and **$H_2$** is used to classify all points in the range above as -1.\n",
    "\n",
    "**$H_1$** cannot generate all $2^N$ dichotomies with $N = 1$, which means $N = 1$ is a breakpoint and $d_{VC} = 0$. Similarly for **$H_2$**.\n",
    "\n",
    "However, when combining **$H_1$** and **$H_2$** together, for the case $N = 1$, it is clear that the new hypothesis created by **$H_1$** and **$H_2$** combined will be able to generate all $2^N = 2$ dichotomies with $N = 1$. This implies that $N = 1$ will no longer be a breakpoint, and $d_{VC}$ at this point will be at least 1.\n",
    "\n",
    "Reassessing $\\sum_{k=1}^{K} d_{VC}(H_k)$, with **$H_1$** and **$H_2$**, we have: $\\sum_{k=1}^{K} d_{VC}(H_k) = \\sum_{k=1}^{2} d_{VC}(H_k) = 0 + 0 = 0$. But we have proven that $d_{VC} \\ge 1$. Therefore, upper bound $\\sum_{k=1}^{K} d_{VC}(H_k)$ is incorrect, so we choose $K - 1 + \\sum_{k=1}^{K} d_{VC}(H_k)$. (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Lower bound:\n",
    "\n",
    "We know that $d_{VC}(H)$ is the largest value of $N$ for which $m_H(N) = 2^N$.\n",
    "\n",
    "We also know $N \\ge 0$. Therefore, $d_{VC} \\ge 0$.\n",
    "\n",
    "The remaining two lower bounds are represented in the answers as $\\min \\{d_{VC}(H_k)\\} _{k=1}^{K}$ and $\\max \\{d_{VC}(H_k)\\} _{k=1}^{K}$, respectively indicating the smallest VC dimension and the largest VC dimension within the set of hypotheses.\n",
    "\n",
    "At this point, we can deduce that the lowest lower bound is 0 since the other two remaining lower bounds could potentially be adjusted to exceed 0, whereas 0 remains a constant value.\n",
    "\n",
    "To determine a lower bound, examine the collection of hypothesis sets, denoted as $\\bigcup_{k=1}^{K} H_k$. Within these sets, identify $H_m$ as the one having the highest VC dimension, indicated as $d_{vc}^{max}$. When assessing the VC dimension of the entire union $d_{vc}(\\bigcup_{k=1}^{K} H_k)$, $H_m$ can consistently shatter $d_{vc}^{max}$ points. Consequently, $d_{vc}^{max}$ functions as a lower bound for $d_{vc}(\\bigcup_{k=1}^{K} H_k)$. (**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "(*)(**) $\\Rightarrow$ Therefore, the answer is **[e] $\\max \\{d_{VC}(H_k)\\} _{k=1}^{K} \\le d_{vc}(\\bigcup_{k=1}^{K} H_k) \\le K - 1 + \\sum_{k=1}^{K} d_{VC}(H_k)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "   [1] Github, Giuliano Mega, Learning From Data - Homework 3, last commit date: 10/04/2018, access date: 26/11/2023, https://rstudio-pubs-static.s3.amazonaws.com/376525_cb5ef7e87d6549af997985b5871fd822.html\n",
    "\n",
    "   [2] Github, Edgardo Deza, Problem 5, last commit date: 18/10/2017, access date: 27/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_3/homework_3_problem_5_Growth_Function.ipynb\n",
    "\n",
    "   [3] Github, Edgardo Deza, Problem 6 & Problem 7 & Problem 8, last commit date: 18/10/2017, access date: 27/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_3/homework_3_problem_6_7_8_Fun_With_Intervals.ipynb\n",
    "\n",
    "   [4] Github, Edgardo Deza, Problem 9, last commit date: 18/10/2017, access date: 27/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_3/homework_3_problem_9_Triangles.ipynb\n",
    "\n",
    "   [5] Github, Edgardo Deza, Problem 10, last commit date: 18/10/2017, access date: 27/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_3/homework_3_problem_10_Concentric_Circles.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
