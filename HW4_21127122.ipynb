{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "\n",
    "Hồ Thanh Nhân - 21127122\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scipy by the reference from this link: https://scipy.org/install/\n",
    "\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [1]\n",
    "\n",
    "For $N > d_{VC}$, we have the approximation: $m_{\\cal{H}}(N) = N^{d_{VC}} \\Leftrightarrow m_{\\cal{H}}(2N) = (2N)^{d_{VC}}$\n",
    "\n",
    "We also have $d_{VC} = 10$\n",
    "\n",
    "\"$95 \\%$ confidence that your generalization error is at most $0.05$\" means: $\\delta = 1 - 0.95 = 0.05$ and $\\epsilon = 0.05$\n",
    "\n",
    "The VC generalization bound is given by: $\\epsilon \\le \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_{\\cal{H}}(2N)}{\\delta}}}$\n",
    "\n",
    "Now, we will find the value of $N$ that satisfies the above inequality, and $N$ is the base to find the closest numerical approximation of the sample size that the VC generalization bound predicts.\n",
    "\n",
    "Let's manipulate the inquality to an equality yields: $\\epsilon = \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_{\\cal{H}}(2N)}{\\delta}}}$\n",
    "\n",
    "$\\Leftrightarrow \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_{\\cal{H}}(2N)}{\\delta}}} - \\epsilon = 0$\n",
    "\n",
    "Let function $f$: $f(N) = \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_{\\cal{H}}(2N)}{\\delta}}} - \\epsilon = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function f(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(N, d_VC, delta, epsilon):\n",
    "    return math.sqrt(8 / N * math.log(4 / delta * (2 * N)**d_VC)) - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of N is:  452957\n"
     ]
    }
   ],
   "source": [
    "# set the intervals (based on the multiple-choice answers)\n",
    "MIN_N = 380000\n",
    "MAX_N = 500000\n",
    "\n",
    "d_VC = 10\n",
    "delta = 0.05\n",
    "epsilon = 0.05\n",
    "root = optimize.brentq(f, MIN_N, MAX_N, args = (d_VC, delta, epsilon))\n",
    "print('Value of N is: ', round(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, the closest numerical approximation of the sample size is [d] 460,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "\n",
    "We have:\n",
    "\n",
    "- $d_{VC} = 50$\n",
    "\n",
    "- $\\delta = 0.05$\n",
    "\n",
    "- $N = 10,000$\n",
    "\n",
    "- $m_{\\cal{H}}(N) = N^{d_{VC}} \\Leftrightarrow m_{\\cal{H}}(2N) = (2N)^{d_{VC}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_VC = 50\n",
    "delta = 0.05\n",
    "N = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the bound in each answer ($f(N)$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [a] Original VC bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_original_VC(N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt(8 / N * math.log(4 / delta * (2 * N) ** d_VC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 0.632174915200836\n"
     ]
    }
   ],
   "source": [
    "print('f(N) =', f_original_VC(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [b] Rademacher Penalty Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Rademacher_Penalty(N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt(2 / N * math.log(2 * N * (N ** d_VC))) + math.sqrt(2 / N * math.log(1 / delta)) + 1 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 0.3313087859616395\n"
     ]
    }
   ],
   "source": [
    "print('f(N) =', f_Rademacher_Penalty(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [c] Parrondo and Van den Broek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Similar to Problem 1, we will consider equality functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Parrondo_and_Van_den_Broek(epsilon, N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt((2 * epsilon + math.log(6 / delta) + d_VC * math.log(2 * N)) / N) - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 0.22369829368078561\n"
     ]
    }
   ],
   "source": [
    "# set the intervals\n",
    "MIN_EPSILON = 0\n",
    "MAX_EPSILON = 1\n",
    "\n",
    "f_Parrondo_and_Van_den_Broek = optimize.brentq(f_Parrondo_and_Van_den_Broek, MIN_EPSILON, MAX_EPSILON, args = (N))\n",
    "print('f(N) =', f_Parrondo_and_Van_den_Broek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [d] Devroye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Similar to Problem 1, we will consider equality functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Devroye(epsilon, N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt((4 * epsilon * (1 + epsilon) + math.log(4 / delta) + 2 * d_VC * math.log(N)) / (2 * N)) - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 0.21522804980824667\n"
     ]
    }
   ],
   "source": [
    "f_Devroye = optimize.brentq(f_Devroye, MIN_EPSILON, MAX_EPSILON, args = (N))\n",
    "print('f(N) =', f_Devroye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, they are not all equal and the smallest bound for $N = 10,000$ is [d] Devroye: $\\epsilon \\le \\sqrt{\\frac{1}{2N}(4 \\epsilon (1 + \\epsilon) + \\ln{\\frac{4 m_{\\cal{H}}(N^2)}{\\delta}})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "\n",
    "We have:\n",
    "\n",
    "- $d_{VC} = 50$\n",
    "\n",
    "- $\\delta = 0.05$\n",
    "\n",
    "- $N = 5$\n",
    "\n",
    "- $m_{\\cal{H}}(N) = N^{d_{VC}} \\Leftrightarrow m_{\\cal{H}}(2N) = (2N)^{d_{VC}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the bound in each answer ($f(N)$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [a] Original VC bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 13.828161484991483\n"
     ]
    }
   ],
   "source": [
    "print('f(N) =', f_original_VC(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [b] Rademacher Penalty Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 7.048776564183685\n"
     ]
    }
   ],
   "source": [
    "print('f(N) =', f_Rademacher_Penalty(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [c] Parrondo and Van den Broek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Similar to Problem 1, we will consider equality functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Parrondo_and_Van_den_Broek_for_P3(epsilon, N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt((2 * epsilon + math.log(6 / delta) + d_VC * math.log(2 * N)) / N) - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 5.101361981989992\n"
     ]
    }
   ],
   "source": [
    "# set the intervals\n",
    "MAX_EPSILON = 20\n",
    "\n",
    "f_Parrondo_and_Van_den_Broek = optimize.brentq(f_Parrondo_and_Van_den_Broek_for_P3, MIN_EPSILON, MAX_EPSILON, args = (N))\n",
    "print('f(N) =', f_Parrondo_and_Van_den_Broek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [d] Devroye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Similar to Problem 1, we will consider equality functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Devroye_for_P3(epsilon, N, d_VC=d_VC, delta=delta):\n",
    "    return math.sqrt((4 * epsilon * (1 + epsilon) + math.log(4 / delta) + 2 * d_VC * math.log(N)) / (2 * N)) - epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(N) = 5.593125543182669\n"
     ]
    }
   ],
   "source": [
    "f_Devroye = optimize.brentq(f_Devroye_for_P3, MIN_EPSILON, MAX_EPSILON, args = (N))\n",
    "print('f(N) =', f_Devroye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, they are not all equal and the smallest bound for $N = 5$ is [c] Parrondo and Van den Broek: $\\epsilon \\le \\sqrt{\\frac{1}{N}(2 \\epsilon + \\ln{\\frac{6 m_{\\cal{H}}(2N)}{\\delta}})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a technique which is called Ordinary Least Squares (OLS).\n",
    "\n",
    "OLS is a type of learning algorithm that can produce a hypothesis (model) that minimizes the mean squared error (MSE) on the examples (training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "The learning model consists of all hypotheses of the form: $h(x) = ax$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize constant\n",
    "NMAX = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_target(x):\n",
    "    return np.sin(np.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_a_hat(N = 2, experiments = NMAX):\n",
    "    total_a = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.array([x_data]).T\n",
    "        \n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        a = weights[0]\n",
    "        \n",
    "        total_a += a\n",
    "\n",
    "    a_hat = total_a / experiments\n",
    "    return round(a_hat, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected value is: g_bar(x) =  1.41 . x\n"
     ]
    }
   ],
   "source": [
    "print('The expected value is: g_bar(x) = ', expected_a_hat(), '. x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the correct answer is **[e] None of the above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    g_bar = expected_a_hat() * x_test\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bias is:  0.3026424541109147\n"
     ]
    }
   ],
   "source": [
    "bias = bias()\n",
    "print('The bias is: ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the closest value to the bias in this case is **[b] 0.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_a_hat = expected_a_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "        x_test = np.random.uniform(-1, 1)\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.array([x_data]).T\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            a = weights[0]\n",
    "\n",
    "            g = a * x_test\n",
    "            g_bar = expected_a_hat * x_test\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance is:  0.2392680784448904\n"
     ]
    }
   ],
   "source": [
    "variance = variance()\n",
    "print('The variance is: ', variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the closest value to the variance in this case is **[a] 0.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\n",
    "$E_{out} = bias + var$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [b] Hypotheses of the form $h(x) = ax$\n",
    "\n",
    "As we analyze in previous Problems, we have the expected value of $E_{out}$ of hypotheses of the form $h(x) = ax$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  0.5419105325558051\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [a] Hypotheses of the form $h(x) = b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_b_hat(N = 2, experiments = NMAX):\n",
    "    total_b = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.ones((N, 1))\n",
    "\n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        b = weights[0]\n",
    "        \n",
    "        total_b += b\n",
    "\n",
    "    b_hat = total_b / experiments\n",
    "    return round(b_hat, 2)\n",
    "\n",
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    g_bar = expected_b_hat()\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset\n",
    "\n",
    "bias = bias()\n",
    "\n",
    "expected_b_hat = expected_b_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.ones((N, 1))\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            b = weights[0]\n",
    "\n",
    "            g = b\n",
    "            g_bar = expected_b_hat\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance\n",
    "\n",
    "variance = variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  0.7649710154522056\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [c] Hypotheses of the form $h(x) = ax + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_weights_hat(N = 2, experiments = NMAX):\n",
    "    total_a = 0\n",
    "    total_b = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.array([x_data, np.ones(N)]).T\n",
    "        \n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        a = weights[0]\n",
    "        b = weights[1]\n",
    "        \n",
    "        total_a += a\n",
    "        total_b += b\n",
    "\n",
    "    a_hat = total_a / experiments\n",
    "    b_hat = total_b / experiments\n",
    "    return round(a_hat, 2), round(b_hat, 2)\n",
    "\n",
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    a_hat, b_hat = expected_weights_hat()\n",
    "    g_bar = a_hat * x_test + b_hat\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset\n",
    "\n",
    "bias = bias()\n",
    "\n",
    "expected_a_hat, expected_b_hat = expected_weights_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "        x_test = np.random.uniform(-1, 1)\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.array([x_data, np.ones(N)]).T\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            a = weights[0]\n",
    "            b = weights[1]\n",
    "\n",
    "            g = a * x_test + b\n",
    "            g_bar = expected_a_hat * x_test + expected_b_hat\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance\n",
    "\n",
    "variance = variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  1.8810110506466935\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [d] Hypotheses of the form $h(x) = ax^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_weights_hat(N = 2, experiments = NMAX):\n",
    "    total_a = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.array([x_data**2]).T\n",
    "        \n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        a = weights[0]\n",
    "        \n",
    "        total_a += a\n",
    "\n",
    "    a_hat = total_a / experiments\n",
    "    return round(a_hat, 2)\n",
    "\n",
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    a_hat = expected_weights_hat()\n",
    "    g_bar = a_hat * x_test**2\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset\n",
    "\n",
    "bias = bias()\n",
    "\n",
    "expected_a_hat = expected_weights_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "        x_test = np.random.uniform(-1, 1)\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.array([x_data**2]).T\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            a = weights[0]\n",
    "\n",
    "            g = a * x_test**2\n",
    "            g_bar = expected_a_hat * x_test**2\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance\n",
    "\n",
    "variance = variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  16.46203867379222\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [e] Hypotheses of the form $h(x) = ax^2 + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_weights_hat(N = 2, experiments = NMAX):\n",
    "    total_a = 0\n",
    "    total_b = 0\n",
    "    for i in range(experiments):\n",
    "        x_data = np.random.uniform(-1, 1, N)\n",
    "        y_data = f_target(x_data)\n",
    "        X_matrix = np.array([x_data**2, np.ones(N)]).T\n",
    "        \n",
    "        # OLS\n",
    "        weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "        a = weights[0]\n",
    "        b = weights[1]\n",
    "        \n",
    "        total_a += a\n",
    "        total_b += b\n",
    "\n",
    "    a_hat = total_a / experiments\n",
    "    b_hat = total_b / experiments\n",
    "    return round(a_hat, 2), round(b_hat, 2)\n",
    "\n",
    "def bias(number_of_test_dataset = NMAX):\n",
    "    x_test = np.random.uniform(-1, 1, number_of_test_dataset)\n",
    "    f_sin = f_target(x_test)\n",
    "    a_hat, b_hat = expected_weights_hat()\n",
    "    g_bar = a_hat * x_test**2 + b_hat\n",
    "    \n",
    "    return np.sum((g_bar - f_sin) ** 2) / number_of_test_dataset\n",
    "\n",
    "bias = bias()\n",
    "\n",
    "expected_a_hat, expected_b_hat = expected_weights_hat()\n",
    "\n",
    "def variance(N = 2, number_of_test_dataset = NMAX, number_of_D = 100):\n",
    "\n",
    "    expectation_D = 0\n",
    "\n",
    "    for _ in range(number_of_test_dataset):\n",
    "        \n",
    "        total_squares = 0\n",
    "        x_test = np.random.uniform(-1, 1)\n",
    "\n",
    "        for _ in range(number_of_D):\n",
    "            x_data = np.random.uniform(-1, 1, N)\n",
    "            y_data = f_target(x_data)\n",
    "            X_matrix = np.array([x_data**2, np.ones(N)]).T\n",
    "\n",
    "            # OLS\n",
    "            weights = np.linalg.inv(X_matrix.T.dot(X_matrix)).dot(X_matrix.T).dot(y_data)\n",
    "\n",
    "            a = weights[0]\n",
    "            b = weights[1]\n",
    "\n",
    "            g = a * x_test**2 + b\n",
    "            g_bar = expected_a_hat * x_test**2 + expected_b_hat\n",
    "\n",
    "            total_squares += (g - g_bar) ** 2\n",
    "\n",
    "        expectation_D += total_squares / number_of_D\n",
    "        \n",
    "    variance = expectation_D / number_of_test_dataset\n",
    "\n",
    "    return variance\n",
    "\n",
    "variance = variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of E_out =  190327.9372684059\n"
     ]
    }
   ],
   "source": [
    "expected_E_out = bias + variance\n",
    "print('Expected value of E_out = ', expected_E_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Therefore, the learning model has the least expected value of out-of-sample error is **[b] Hypotheses of the form $h(x) = ax$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "   [1] Github, Giuliano Mega, Learning From Data - Homework 3, last commit date: 10/04/2018, access date: 26/11/2023, https://rstudio-pubs-static.s3.amazonaws.com/376525_cb5ef7e87d6549af997985b5871fd822.html\n",
    "\n",
    "   [2] Github, Edgardo Deza, Problem 5, last commit date: 18/10/2017, access date: 27/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_3/homework_3_problem_5_Growth_Function.ipynb\n",
    "\n",
    "   [3] Github, Edgardo Deza, Problem 6 & Problem 7 & Problem 8, last commit date: 18/10/2017, access date: 27/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_3/homework_3_problem_6_7_8_Fun_With_Intervals.ipynb\n",
    "\n",
    "   [4] Github, Edgardo Deza, Problem 9, last commit date: 18/10/2017, access date: 27/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_3/homework_3_problem_9_Triangles.ipynb\n",
    "\n",
    "   [5] Github, Edgardo Deza, Problem 10, last commit date: 18/10/2017, access date: 27/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_3/homework_3_problem_10_Concentric_Circles.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
