{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab03: Logistic Regression.\n",
    "\n",
    "- Student ID: 21127122\n",
    "- Student name: Hồ Thanh Nhân"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`). Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "- Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\IT\\VSCODE\\__PYTHON__\\Machine-Learning-HW\\Lab03\\Lab03-LogisticRegression.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IT/VSCODE/__PYTHON__/Machine-Learning-HW/Lab03/Lab03-LogisticRegression.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_openml\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/IT/VSCODE/__PYTHON__/Machine-Learning-HW/Lab03/Lab03-LogisticRegression.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X, y \u001b[39m=\u001b[39m fetch_openml(\u001b[39m'\u001b[39;49m\u001b[39mmnist_784\u001b[39;49m\u001b[39m'\u001b[39;49m, return_X_y\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:1128\u001b[0m, in \u001b[0;36mfetch_openml\u001b[1;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# obtain the data\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m url \u001b[39m=\u001b[39m _DATA_FILE\u001b[39m.\u001b[39mformat(data_description[\u001b[39m\"\u001b[39m\u001b[39mfile_id\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1128\u001b[0m bunch \u001b[39m=\u001b[39m _download_data_to_bunch(\n\u001b[0;32m   1129\u001b[0m     url,\n\u001b[0;32m   1130\u001b[0m     return_sparse,\n\u001b[0;32m   1131\u001b[0m     data_home,\n\u001b[0;32m   1132\u001b[0m     as_frame\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(as_frame),\n\u001b[0;32m   1133\u001b[0m     openml_columns_info\u001b[39m=\u001b[39;49mfeatures_list,\n\u001b[0;32m   1134\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1135\u001b[0m     target_columns\u001b[39m=\u001b[39;49mtarget_columns,\n\u001b[0;32m   1136\u001b[0m     data_columns\u001b[39m=\u001b[39;49mdata_columns,\n\u001b[0;32m   1137\u001b[0m     md5_checksum\u001b[39m=\u001b[39;49mdata_description[\u001b[39m\"\u001b[39;49m\u001b[39mmd5_checksum\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1138\u001b[0m     n_retries\u001b[39m=\u001b[39;49mn_retries,\n\u001b[0;32m   1139\u001b[0m     delay\u001b[39m=\u001b[39;49mdelay,\n\u001b[0;32m   1140\u001b[0m     parser\u001b[39m=\u001b[39;49mparser_,\n\u001b[0;32m   1141\u001b[0m     read_csv_kwargs\u001b[39m=\u001b[39;49mread_csv_kwargs,\n\u001b[0;32m   1142\u001b[0m )\n\u001b[0;32m   1144\u001b[0m \u001b[39mif\u001b[39;00m return_X_y:\n\u001b[0;32m   1145\u001b[0m     \u001b[39mreturn\u001b[39;00m bunch\u001b[39m.\u001b[39mdata, bunch\u001b[39m.\u001b[39mtarget\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:677\u001b[0m, in \u001b[0;36m_download_data_to_bunch\u001b[1;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m ParserError\n\u001b[0;32m    675\u001b[0m     no_retry_exception \u001b[39m=\u001b[39m ParserError\n\u001b[1;32m--> 677\u001b[0m X, y, frame, categories \u001b[39m=\u001b[39m _retry_with_clean_cache(\n\u001b[0;32m    678\u001b[0m     url, data_home, no_retry_exception\n\u001b[0;32m    679\u001b[0m )(_load_arff_response)(\n\u001b[0;32m    680\u001b[0m     url,\n\u001b[0;32m    681\u001b[0m     data_home,\n\u001b[0;32m    682\u001b[0m     parser\u001b[39m=\u001b[39;49mparser,\n\u001b[0;32m    683\u001b[0m     output_type\u001b[39m=\u001b[39;49moutput_type,\n\u001b[0;32m    684\u001b[0m     openml_columns_info\u001b[39m=\u001b[39;49mfeatures_dict,\n\u001b[0;32m    685\u001b[0m     feature_names_to_select\u001b[39m=\u001b[39;49mdata_columns,\n\u001b[0;32m    686\u001b[0m     target_names_to_select\u001b[39m=\u001b[39;49mtarget_columns,\n\u001b[0;32m    687\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    688\u001b[0m     md5_checksum\u001b[39m=\u001b[39;49mmd5_checksum,\n\u001b[0;32m    689\u001b[0m     n_retries\u001b[39m=\u001b[39;49mn_retries,\n\u001b[0;32m    690\u001b[0m     delay\u001b[39m=\u001b[39;49mdelay,\n\u001b[0;32m    691\u001b[0m     read_csv_kwargs\u001b[39m=\u001b[39;49mread_csv_kwargs,\n\u001b[0;32m    692\u001b[0m )\n\u001b[0;32m    694\u001b[0m \u001b[39mreturn\u001b[39;00m Bunch(\n\u001b[0;32m    695\u001b[0m     data\u001b[39m=\u001b[39mX,\n\u001b[0;32m    696\u001b[0m     target\u001b[39m=\u001b[39my,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m     target_names\u001b[39m=\u001b[39mtarget_columns,\n\u001b[0;32m    701\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:67\u001b[0m, in \u001b[0;36m_retry_with_clean_cache.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m     66\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m     68\u001b[0m \u001b[39mexcept\u001b[39;00m URLError:\n\u001b[0;32m     69\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:512\u001b[0m, in \u001b[0;36m_load_arff_response\u001b[1;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay, read_csv_kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_arff_response\u001b[39m(\n\u001b[0;32m    430\u001b[0m     url: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    431\u001b[0m     data_home: Optional[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     read_csv_kwargs: Optional[Dict] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m ):\n\u001b[0;32m    443\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load the ARFF data associated with the OpenML URL.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[39m    In addition of loading the data, this function will also check the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39m        `output_array_type == \"pandas\"`.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 512\u001b[0m     gzip_file \u001b[39m=\u001b[39m _open_openml_url(url, data_home, n_retries\u001b[39m=\u001b[39;49mn_retries, delay\u001b[39m=\u001b[39;49mdelay)\n\u001b[0;32m    513\u001b[0m     \u001b[39mwith\u001b[39;00m closing(gzip_file):\n\u001b[0;32m    514\u001b[0m         md5 \u001b[39m=\u001b[39m hashlib\u001b[39m.\u001b[39mmd5()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:183\u001b[0m, in \u001b[0;36m_open_openml_url\u001b[1;34m(openml_path, data_home, n_retries, delay)\u001b[0m\n\u001b[0;32m    181\u001b[0m                 opener \u001b[39m=\u001b[39m gzip\u001b[39m.\u001b[39mGzipFile\n\u001b[0;32m    182\u001b[0m             \u001b[39mwith\u001b[39;00m opener(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(tmpdir, file_name), \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[1;32m--> 183\u001b[0m                 shutil\u001b[39m.\u001b[39;49mcopyfileobj(fsrc, fdst)\n\u001b[0;32m    184\u001b[0m         shutil\u001b[39m.\u001b[39mmove(fdst\u001b[39m.\u001b[39mname, local_path)\n\u001b[0;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\shutil.py:197\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    195\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[0;32m    196\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[0;32m    199\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:460\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_chunked(amt)\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m         \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:592\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m=\u001b[39m chunk_left \u001b[39m-\u001b[39m amt\n\u001b[0;32m    590\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m value\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(chunk_left))\n\u001b[0;32m    593\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m     amt \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m chunk_left\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[0;32m    625\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[0;32m    633\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we basically have 70000 samples with each sample having 784 features - pixels in this case and a label - the digit the image represent.\n",
    "\n",
    "Let’s play around and see if we can extract any features from the pixels that can be more informative. First I’d like to know more about average intensity - that is the average value of a pixel in an image for the different digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.unique(y)\n",
    "# print(labels)\n",
    "n_label=np.unique(y).shape[0]\n",
    "l_means=np.zeros(shape=n_label,dtype=float) #array stores average intensity for each label\n",
    "\n",
    "#TODO compute average intensity for each label\n",
    "\n",
    "# print(l_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average intensity using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels,l_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some differences in intensity. The digit “1” is the less intense while the digit “0” is the most intense. So this new feature seems to have some predictive value if you wanted to know if say your digit is a “1” or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO compute average intensity for each data sample\n",
    "# intensity=?\n",
    "print(intensity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes people really do not know what are they doing. I am not an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flip=np.flip(X)\n",
    "symmetry= np.mean((X-X_flip),axis=1)\n",
    "print(symmetry.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I called this feature \"symmetry\" (though it's not \"symmetry\" at all). Use visualization method to understand why this feature work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new trainning data will have 70000 samples and 2 features: intensity, symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO create X_new by horizontal stack intensity and symmetry\n",
    "print(X_new.shape) #it should be (70000,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually logistic regression is a good first choice for classification. In this homework we use logistic regression for classifying digit 1 images and not digit 1 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data\n",
    "First normalize data using Z-score normalization\n",
    "- **TODO: Study about Z-score normalization**\n",
    "- **TODO: Why should we normalize data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: normalize X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.hstack((np.ones((len(X_new), 1)), X_new)) #stack 1s column as usual\n",
    "y_new=y.astype(int)\n",
    "y_new[y_new != 1] = 0 # digit 1 -> class 1, other digits -> class 0\n",
    "y_new=y_new.reshape(-1,1)\n",
    "print (X_new.shape)\n",
    "print (y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X_new, y_new, test_size= int(1/3*X.shape[0]))\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function and derivative of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    \"\"\"compute the sigmoid activation value for a given input\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "def sigmoid_deriv(x):\n",
    "    '''compute the derivative of the sigmoid function ASSUMING\n",
    "    that the input ‘x‘ has already been passed through the sigmoid\n",
    "    activation function'''\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h(W, X):\n",
    "    \"\"\"\n",
    "    Compute output: Take the dot product between our features ‘X‘ and the weight\n",
    "    matrix ‘W‘, then pass this value through our sigmoid activation function \n",
    "    \"\"\"\n",
    "    return sigmoid_activation(X.dot(W))\n",
    "def predict(W, X):\n",
    " \n",
    "    '''Take the dot product between our features and weight matrix, \n",
    "       then pass this value through our sigmoid activation'''\n",
    "    #........\n",
    "    preds=sigmoid_activation(X.dot(W))\n",
    "    # apply a step function to threshold the outputs to binary\n",
    "    # class labels\n",
    "    preds[preds <= 0.5] = 0\n",
    "    preds[preds > 0] = 1\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function: Average negative log likelihood**\n",
    "$$\\mathcal{L}=\\dfrac{1}{N} \\sum_{i=1}^{N} -\\left(y^{i}\\ln h_{\\mathbf{w}}\\left(\\mathbf{x}^{i}\\right)+\\left(1-y^{i}\\right)\\ln \\left(1-h_{\\mathbf{w}}\\left(x^{i}\\right)\\right)\\right) $$\n",
    "\n",
    "\n",
    "$$\\text{Sigmoid Activation: } z= \\sigma \\left(h\\right)= \\dfrac{1}{1+e^{-h}}$$\n",
    "\n",
    "$$\\text{Cross-entropy: } J(w)=-\\left({ylog(z)+(1-y)log(1-z)}\\right)$$\n",
    "\n",
    "$$\\text{Chain rule: } \\dfrac{\\partial J(w)}{\\partial w}=\\dfrac{\\partial J(w)}{\\partial z} \\dfrac{\\partial z}{\\partial h}\\dfrac{\\partial h}{\\partial w}  $$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial z}=-\\left(\\dfrac{y}{z}-\\dfrac{1-y}{1-z}\\right)=\\dfrac{z-y}{z(1-z)}$$\n",
    "\n",
    "$$\\dfrac{\\partial z}{\\partial h}=z(1-z)$$\n",
    "\n",
    "$$\\dfrac{\\partial h}{\\partial w}=X$$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial w}=X^T(z-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(train_X, error):\n",
    "    \"\"\"\n",
    "    This is the gradient descent update of \"average negative loglikelihood\" loss function. \n",
    "    In lab02 our loss function is \"sum squared error\".\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W,train_X, train_y, learning_rate, num_epochs, losses):\n",
    "    for epoch in np.arange(0, num_epochs):\n",
    "        h=compute_h(W,train_X)\n",
    "        error = h - train_y\n",
    "        loss = np.mean(- train_y * np.log(h) - (1 - train_y) * np.log(1 - h))\n",
    "        losses.append(loss)\n",
    "        gradient=compute_gradient(h, error)\n",
    "        W += -learning_rate * gradient\n",
    "        if ((epoch+1)%1000==0): print ('Epoch %d, loss %.3f' %(epoch+1, loss))\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W = np.random.randn(train_X.shape[1], 1)\n",
    "losses=[]\n",
    "num_epochs=40000\n",
    "learning_rate=0.01\n",
    "W=train(W,train_X, train_y, learning_rate, num_epochs , losses)\n",
    "x_preds=predict(W ,train_X)\n",
    "train_err = np.mean(x_preds != train_y) * 100\n",
    "print ('=' * 50)\n",
    "print ('Train err of final w: ', train_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(W, train_X)\n",
    "print(classification_report(train_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(W, test_X)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Comment on the result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
