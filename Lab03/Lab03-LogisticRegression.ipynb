{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab03: Logistic Regression.\n",
    "\n",
    "- Student ID: 21127122\n",
    "- Student name: Hồ Thanh Nhân"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`). Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "- Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', return_X_y=True, parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we basically have 70000 samples with each sample having 784 features - pixels in this case and a label - the digit the image represent.\n",
    "\n",
    "Let’s play around and see if we can extract any features from the pixels that can be more informative. First I’d like to know more about average intensity - that is the average value of a pixel in an image for the different digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44.17740512 19.40680177 38.03420776 36.15420938 30.99599983 32.95015873\n",
      " 35.23486491 29.21798737 38.39790125 31.35940809]\n"
     ]
    }
   ],
   "source": [
    "labels=np.unique(y)\n",
    "# print(labels)\n",
    "n_label=np.unique(y).shape[0]\n",
    "l_means=np.zeros(shape=n_label,dtype=float) #array stores average intensity for each label\n",
    "\n",
    "#TODO compute average intensity for each label\n",
    "\n",
    "for label in labels:\n",
    "    l_means[int(label)] = np.mean(X[label == y])\n",
    "    \n",
    "print(l_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average intensity using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels,l_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some differences in intensity. The digit “1” is the less intense while the digit “0” is the most intense. So this new feature seems to have some predictive value if you wanted to know if say your digit is a “1” or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO compute average intensity for each data sample\n",
    "intensity = np.mean(X, axis=1)\n",
    "print(intensity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes people really do not know what are they doing. I am not an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X.values)\n",
    "X_flip=np.flip(X, axis=1)\n",
    "symmetry= np.mean(abs(X-X_flip), axis=1)\n",
    "print(symmetry.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I called this feature \"symmetry\" (though it's not \"symmetry\" at all). Use visualization method to understand why this feature work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new trainning data will have 70000 samples and 2 features: intensity, symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2)\n"
     ]
    }
   ],
   "source": [
    "#TODO create X_new by horizontal stack intensity and symmetry\n",
    "X_new = np.column_stack((intensity, symmetry))\n",
    "print(X_new.shape) #it should be (70000,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually logistic regression is a good first choice for classification. In this homework we use logistic regression for classifying digit 1 images and not digit 1 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data\n",
    "First normalize data using Z-score normalization\n",
    "- **TODO: Study about Z-score normalization**\n",
    "- **TODO: Why should we normalize data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Z-Score Normalization** [1] [2]\n",
    "\n",
    "Z-score normalization, also known as standard score normalization, is a technique used in statistics and machine learning to standardize the scale of features or variables. The goal is to transform the data such that the mean of the values is 0 and the standard deviation is 1.\n",
    "\n",
    "We use the following formula to perform a z-score normalization on every value in a dataset:\n",
    "\n",
    "$$Z\\ Score = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$x: Original\\ value$\n",
    "\n",
    "$\\mu: Mean\\ of\\ data$\n",
    "\n",
    "$\\sigma: Standard\\ deviation\\ of\\ data$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Why should we normalize data?** [3] [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing a vector most often means dividing by a norm of the vector. It also often refers to rescaling by the minimum and range of the vector, to make all the elements lie between 0 and 1 thus bringing all the values of numeric columns in the dataset to a common scale.\n",
    "\n",
    "Data normalization is a crucial step in data preprocessing, particularly in machine learning and statistics.\n",
    "\n",
    "The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. It is required only when features have different ranges.\n",
    "\n",
    "- For example, consider a data set containing two features, age, and income(x2). Where age ranges from 0–100, while income ranges from 0–100,000 and higher. Income is about 1,000 times larger than age. So, these two features are in very different ranges. When we do further analysis, like multivariate linear regression, for example, the attributed income will intrinsically influence the result more due to its larger value. But this doesn’t necessarily mean it is more important as a predictor. So we normalize the data to bring all the variables to the same range.\n",
    "\n",
    "**In Machine Learning:**\n",
    "\n",
    "- Normalization in machine learning plays a really important part within the precision of calculations. Normalization makes a difference to scale features so that the information is inside a certain range, usually between 0 and 1. This guarantees that all features contribute equally to the analysis, otherwise it might lead to bias towards one include.\n",
    "\n",
    "- Normalization in machine learning too helps in increasing the convergence rate of machine learning algorithms such as clustering, neural networks, and regression. Typically since the algorithms work better when the data points are near to each other and inside the same range. With normalization, the data points are more homogenous and the machine learning algorithm can learn and make more accurate predictions.\n",
    "\n",
    "- Additionally, normalization makes a difference to decrease the sum of noise in the data. When the information is centered around a mean of zero, it'll be simpler to identify the vital designs and relationships. This will lead to better comes about and more exact predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: normalize X_new\n",
    "mean = np.mean(X_new, axis=0)\n",
    "standard_deviation = np.std(X_new, axis=0)\n",
    "X_new = (X_new - mean) / standard_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 3)\n",
      "(70000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_new = np.hstack((np.ones((len(X_new), 1)), X_new)) #stack 1s column as usual\n",
    "y_new=y.astype(int)\n",
    "y_new[y_new != 1] = 0 # digit 1 -> class 1, other digits -> class 0\n",
    "y_new=np.array(y_new).reshape(-1,1)\n",
    "print (X_new.shape)\n",
    "print (y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46667, 3)\n",
      "(46667, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X_new, y_new, test_size= int(1/3*X.shape[0]))\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function and derivative of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    \"\"\"compute the sigmoid activation value for a given input\"\"\"\n",
    "    # Clip the input values to a specific range to prevent overflow\n",
    "    clipped_x = np.clip(x, -500, 500)\n",
    "    \n",
    "    # Compute the sigmoid using the clipped input (clipped_x)\n",
    "    return 1.0 / (1 + np.exp(-clipped_x))\n",
    "def sigmoid_deriv(x):\n",
    "    '''compute the derivative of the sigmoid function ASSUMING\n",
    "    that the input ‘x‘ has already been passed through the sigmoid\n",
    "    activation function'''\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h(W, X):\n",
    "    \"\"\"\n",
    "    Compute output: Take the dot product between our features ‘X‘ and the weight\n",
    "    matrix ‘W‘, then pass this value through our sigmoid activation function \n",
    "    \"\"\"\n",
    "    return sigmoid_activation(X.dot(W))\n",
    "def predict(W, X):\n",
    " \n",
    "    '''Take the dot product between our features and weight matrix, \n",
    "       then pass this value through our sigmoid activation'''\n",
    "    #........\n",
    "    preds=sigmoid_activation(X.dot(W))\n",
    "    # apply a step function to threshold the outputs to binary\n",
    "    # class labels\n",
    "    preds[preds <= 0.5] = 0\n",
    "    preds[preds > 0] = 1\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function: Average negative log likelihood**\n",
    "$$\\mathcal{L}=\\dfrac{1}{N} \\sum_{i=1}^{N} -\\left(y^{i}\\ln h_{\\mathbf{w}}\\left(\\mathbf{x}^{i}\\right)+\\left(1-y^{i}\\right)\\ln \\left(1-h_{\\mathbf{w}}\\left(x^{i}\\right)\\right)\\right) $$\n",
    "\n",
    "\n",
    "$$\\text{Sigmoid Activation: } z= \\sigma \\left(h\\right)= \\dfrac{1}{1+e^{-h}}$$\n",
    "\n",
    "$$\\text{Cross-entropy: } J(w)=-\\left({ylog(z)+(1-y)log(1-z)}\\right)$$\n",
    "\n",
    "$$\\text{Chain rule: } \\dfrac{\\partial J(w)}{\\partial w}=\\dfrac{\\partial J(w)}{\\partial z} \\dfrac{\\partial z}{\\partial h}\\dfrac{\\partial h}{\\partial w}  $$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial z}=-\\left(\\dfrac{y}{z}-\\dfrac{1-y}{1-z}\\right)=\\dfrac{z-y}{z(1-z)}$$\n",
    "\n",
    "$$\\dfrac{\\partial z}{\\partial h}=z(1-z)$$\n",
    "\n",
    "$$\\dfrac{\\partial h}{\\partial w}=X$$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial w}=X^T(z-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(train_X, error):\n",
    "    \"\"\"\n",
    "    This is the gradient descent update of \"average negative loglikelihood\" loss function. \n",
    "    In lab02 our loss function is \"sum squared error\".\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    gradient = train_X.T.dot(error)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W,train_X, train_y, learning_rate, num_epochs, losses):\n",
    "    # fixing \"divide by zero encountered in log\"\n",
    "    epsilon = 1e-16\n",
    "    train_y = np.clip(train_y, epsilon, 1 - epsilon)\n",
    "    for epoch in np.arange(0, num_epochs):\n",
    "        h=compute_h(W,train_X)\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        error = h - train_y\n",
    "        loss = np.mean(- train_y * np.log(h) - (1 - train_y) * np.log(1 - h))\n",
    "        losses.append(loss)\n",
    "        gradient=compute_gradient(train_X, error)\n",
    "        W += -learning_rate * gradient\n",
    "        if ((epoch+1)%1000==0): print ('Epoch %d, loss %.3f' %(epoch+1, loss))\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, loss 2.141\n",
      "Epoch 2000, loss 2.141\n",
      "Epoch 3000, loss 2.141\n",
      "Epoch 4000, loss 2.141\n",
      "Epoch 5000, loss 2.141\n",
      "Epoch 6000, loss 2.141\n",
      "Epoch 7000, loss 2.141\n",
      "Epoch 8000, loss 2.141\n",
      "Epoch 9000, loss 2.141\n",
      "Epoch 10000, loss 2.141\n",
      "Epoch 11000, loss 2.141\n",
      "Epoch 12000, loss 2.141\n",
      "Epoch 13000, loss 2.141\n",
      "Epoch 14000, loss 2.141\n",
      "Epoch 15000, loss 2.141\n",
      "Epoch 16000, loss 2.141\n",
      "Epoch 17000, loss 2.141\n",
      "Epoch 18000, loss 2.141\n",
      "Epoch 19000, loss 2.141\n",
      "Epoch 20000, loss 2.141\n",
      "Epoch 21000, loss 2.141\n",
      "Epoch 22000, loss 2.141\n",
      "Epoch 23000, loss 2.141\n",
      "Epoch 24000, loss 2.141\n",
      "Epoch 25000, loss 2.141\n",
      "Epoch 26000, loss 2.141\n",
      "Epoch 27000, loss 2.141\n",
      "Epoch 28000, loss 2.141\n",
      "Epoch 29000, loss 2.141\n",
      "Epoch 30000, loss 2.141\n",
      "Epoch 31000, loss 2.141\n",
      "Epoch 32000, loss 2.141\n",
      "Epoch 33000, loss 2.141\n",
      "Epoch 34000, loss 2.141\n",
      "Epoch 35000, loss 2.141\n",
      "Epoch 36000, loss 2.141\n",
      "Epoch 37000, loss 2.141\n",
      "Epoch 38000, loss 2.141\n",
      "Epoch 39000, loss 2.141\n",
      "Epoch 40000, loss 2.141\n",
      "==================================================\n",
      "Train err of final w:  6.634238326869093\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(train_X.shape[1], 1)\n",
    "losses=[]\n",
    "num_epochs=40000\n",
    "learning_rate=0.01\n",
    "W=train(W,train_X, train_y, learning_rate, num_epochs , losses)\n",
    "x_preds=predict(W ,train_X)\n",
    "train_err = np.mean(x_preds != train_y) * 100\n",
    "print ('=' * 50)\n",
    "print ('Train err of final w: ', train_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     41373\n",
      "           1       0.59      0.92      0.72      5294\n",
      "\n",
      "    accuracy                           0.92     46667\n",
      "   macro avg       0.79      0.92      0.84     46667\n",
      "weighted avg       0.94      0.92      0.93     46667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(W, train_X)\n",
    "print(classification_report(train_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     20750\n",
      "           1       0.59      0.93      0.72      2583\n",
      "\n",
      "    accuracy                           0.92     23333\n",
      "   macro avg       0.79      0.92      0.84     23333\n",
      "weighted avg       0.95      0.92      0.93     23333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(W, test_X)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Comment on the result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Logistic Regression model employed is a binary classification algorithm utilized to classify handwritten digits within the widely used MNIST_784 dataset. This dataset consists of 70,000 grayscale images depicting digits ranging from 0 to 9, each with dimensions of 28x28 pixels. The features used for training are obtained by horizontally stacking attributes related to intensity and symmetry extracted from the dataset. To standardize the values of these features, the Z-score normalization technique is applied. The model is trained using the Logistic Regression algorithm with gradient descent optimization.\n",
    "\n",
    "- The Logistic Regression model aims to classify handwritten digits into two categories: digit \"1\" and non-digit \"0\". By leveraging features extracted from intensity and symmetry attributes, the model attempts to differentiate between these categories based on the given dataset.\n",
    "\n",
    "- The evaluation of the Logistic Regression model on both the training and test datasets demonstrates consistent performance across various metrics. For the training dataset, class 0 exhibits high precision, recall, and F1-scores (99%, 92%, 95% respectively), while class 1 shows moderate precision (59%) but strong recall and F1-score (92%, 72% respectively). The overall accuracy on the training set stands at 92%. Similarly, on the test dataset, class 0 maintains high precision, recall, and F1-scores (99%, 92%, 95%), while class 1 displays slightly lower precision (59%) yet strong recall and F1-score (93%, 72%). The overall accuracy on the test set remains consistent at 92%.\n",
    "\n",
    "**Model Performance:**\n",
    "\n",
    "- The training dataset:\n",
    "\n",
    "    - Accuracy: The model achieves an accuracy of 92% on the training data, indicating that it correctly predicts the classes for 92% of the samples.\n",
    "    - Precision and Recall: For class 0, the model shows high precision (99%) and recall (92%), suggesting that it accurately identifies the true positives while minimizing false positives. Class 1, however, displays lower precision (59%) but significantly higher recall (92%), indicating it can correctly identify a higher proportion of actual positive cases among those predicted as positive.\n",
    "    - F1-score: The F1-score for class 0 is 95%, implying a good balance between precision and recall. Class 1 has an F1-score of 72%, which indicates a relatively lower balance between precision and recall compared to class 0.\n",
    "\n",
    "- The test dataset:\n",
    "    - The model exhibits similar performance on the test dataset compared to the training set, maintaining an accuracy of 92%.\n",
    "    - Precision, Recall, and F1-score: The precision, recall, and F1-scores for both classes (0 and 1) are consistent with those of the training set, indicating that the model generalizes well to unseen data.\n",
    "    - Balanced Metrics: While the precision for class 1 remains at 59%, its recall is notably high at 93%, suggesting the model effectively identifies most of the positive cases, albeit with some misclassifications.\n",
    "\n",
    "    $\\Rightarrow$ The consistency in performance metrics between the training and test datasets suggests that the model doesn't suffer from overfitting.\n",
    "\n",
    "    $\\Rightarrow$ The Logistic Regression model performed well on both training and test data, showcasing consistent metrics for reliability.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "- Strengths:\n",
    "    - Strong performance with high accuracy on both training and test data.\n",
    "    - Good precision and recall for class 0, indicating accurate predictions for the majority class.\n",
    "    - Decent recall for class 1, correctly identifying instances of the minority class.\n",
    "\n",
    "- Weaknesses:\n",
    "    - Lower precision for class 1 suggests misclassification and false positives for the minority class.\n",
    "    - Class imbalance might affect model performance, especially for class 1.\n",
    "\n",
    "$\\Rightarrow$ Overall, the model performs well but struggles slightly with precision for class 1, indicating room for enhancement, particularly in handling the imbalanced classes for a more balanced predictive outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "   [1] statology.org, ZACH, Z-Score Normalization: Definition & Examples, last updated: 12/08/2021, access date: 28/11/2023, https://www.statology.org/z-score-normalization/\n",
    "\n",
    "   [2] ML Concepts, Z-Score Normalization, last updated: 23/11/2022, access date: 28/11/2023, https://www.linkedin.com/pulse/z-score-normalization-ml-concepts-com\n",
    "\n",
    "   [3] towardsai.net, Editorial Team, How, When, and Why Should You Normalize / Standardize / Rescale Your Data?, last updated: 28/05/2020, access date: 28/11/2023, https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff\n",
    "\n",
    "   [4] almabetter.com, Data Normalization in Machine Learning, access date: 28/11/2023, https://www.almabetter.com/bytes/tutorials/data-science/normalization-in-machine-learning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
