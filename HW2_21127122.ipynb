{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "\n",
    "Hồ Thanh Nhân - 21127122\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoeffding Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000 # 1000 virtual fair coins\n",
    "T = 10 # 10 times of flipping each coin independently\n",
    "M = 100000 # 100,000 times of running the experiment\n",
    "states = ['H', 'T'] # 2 states of flipping coin. Head and Tail\n",
    "coins = {}\n",
    "\n",
    "# Contants\n",
    "EPSILON = 0.4 # large tolerance because sample size is small\n",
    "E_OUT = 0.5 # law of large number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement experiment for M time(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(M = 1):\n",
    "    v_1: List[float] = []\n",
    "    v_rand: List[float] = []\n",
    "    v_min: List[float] = []\n",
    "    for j in range(M):\n",
    "        # Flipping N virtual fair coins. Flip each coin independently T times.\n",
    "        coins = np.random.choice(['H', 'T'], size=(N, T))\n",
    "\n",
    "        # The first coin flipped\n",
    "        c_1 = coins[0]\n",
    "\n",
    "        # A coin chosen randomly from N coins\n",
    "        c_rand = coins[np.random.randint(N)]\n",
    "\n",
    "        # c_min is the coin which had the minimum frequency of heads\n",
    "        min_head_index = np.argmin(np.sum(coins == 'H', axis = 1))\n",
    "        c_min = coins[min_head_index]\n",
    "\n",
    "        # v_1, v_rand, v_min (the fraction of heads obtained for the 3 respective coins out of the 10 tosses)\n",
    "        v_1.append(np.sum(c_1 == 'H') / T)\n",
    "        v_rand.append(np.sum(c_rand == 'H') / T)\n",
    "        v_min.append(np.sum(c_min == 'H') / T)\n",
    "\n",
    "    return np.array(v_1), np.array(v_rand), np.array(v_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiment 100,000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a full distribution of v_1, v_rand and v_min\n",
    "v_1_list, v_rand_list, v_min_list = experiments(M) # M = 100,000\n",
    "\n",
    "# Calculate average value for v_1, v_rand, v_min\n",
    "v_1 = np.sum(v_1_list) / M\n",
    "v_rand = np.sum(v_rand_list) / M\n",
    "v_min = np.sum(v_min_list) / M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average value of v_min is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037797"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the closest answer is [b] 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the right side of Hoeffding Inequality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoeffding_bound = 2 * np.exp(-2 * EPSILON**2 * T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check coin(s) has a distribution of v that satisfies the (single-bin) Hoeffding Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def satisfy_Hoeffding_Inequality(v):\n",
    "    return np.sum(np.abs(v - E_OUT) > EPSILON) / M < hoeffding_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check v_1\n",
    "print(satisfy_Hoeffding_Inequality(v_1_list))\n",
    "\n",
    "# Check v_rand\n",
    "print(satisfy_Hoeffding_Inequality(v_rand_list))\n",
    "\n",
    "# Check v_min\n",
    "print(satisfy_Hoeffding_Inequality(v_min_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the answer is [d] c1 and crand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error and Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "Notation:\n",
    "\n",
    "$h$: hypothesis\n",
    "\n",
    "$f$: target function\n",
    "\n",
    "$\\mu$: probability that $h$ is wrong\n",
    "\n",
    "$\\lambda$: probability that $y = f(x)$, that is no noise\n",
    "\n",
    "$h$ and $f$ are binary functions (their output is 0 or 1).\n",
    "\n",
    "There are 2 posibilities of error that h makes in approximating y:\n",
    "- $h$ makes an error with probability $\\mu$ but $y$ receives the correct value ($y = f(x)$) with probability $\\lambda$.\n",
    "- $h$ doesn't make error with probability ($1 - \\mu$) but $y$ receives the incorrect value ($y \\neq f(x)$) with probability ($1 - \\lambda$).\n",
    "\n",
    "In total, we get:\n",
    "\n",
    "$(1 - \\lambda) * (1 - \\mu) + \\lambda * \\mu$\n",
    "\n",
    "Therefore, the answer is [e] $(1 - \\lambda) * (1 - \\mu) + \\lambda * \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "We have: $P(error) = (1 - \\lambda) * (1 - \\mu) + \\lambda * \\mu$ (Q3)\n",
    "\n",
    "$\\Leftrightarrow P(error) = 1 - \\mu - \\lambda + 2 * \\mu * \\lambda$\n",
    "\n",
    "$\\Leftrightarrow P(error) = 1 - \\lambda + \\mu * (2 * \\lambda - 1)$\n",
    "\n",
    "If $\\lambda = \\frac{1}{2}$, we get $P(error) = \\frac{1}{2}$ which is a constant value.\n",
    "\n",
    "Therefore, with $\\lambda = \\frac{1}{2}$, the performance of h will be independent of $\\mu$\n",
    "\n",
    "The answer is [b] 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_POINTS = 100\n",
    "NUM_TEST_POINTS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating `target_w`, the vector of parameters of $f$ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_target_w():\n",
    "    \"\"\"\n",
    "    Generates target_w from two random, uniformly distributed points in [-1, 1] x [-1, 1].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target_w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of f.\n",
    "    \"\"\"\n",
    "    # Generate two points from a uniform distribution over [-1, 1]x[-1, 1]\n",
    "    p1 = np.random.uniform(-1, 1, 2)\n",
    "    p2 = np.random.uniform(-1, 1, 2)\n",
    "    # Compute the target W from these two points\n",
    "    target_w = np.array([p1[1] * p2[0] - p1[0] * p2[1], p2[1] - p1[1], p1[0] - p2[0]]).reshape((-1, 1))\n",
    "    \n",
    "    return target_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(N, target_w):\n",
    "    \"\"\"\n",
    "    Generates a data set by generating random inputs and then using target_w to generate the \n",
    "    corresponding outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of examples.\n",
    "    target_w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of f.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.        \n",
    "    \"\"\"\n",
    "    X = np.random.uniform(-1, 1, (N, 2))\n",
    "    X = np.hstack((np.ones((N, 1)), X)) # Add 'ones' column\n",
    "    Y = np.sign(np.dot(X, target_w))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_Linear_Regression_train(X, Y):\n",
    "    model = LinearRegression().fit(X, Y.reshape(-1))\n",
    "    E_in = np.sum(np.sign(model.predict(X)) != Y.reshape(-1)) / len(X)\n",
    "    return model, E_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Linear_Regression_test(X_test,  Y_test, model):\n",
    "    E_out = np.sum(np.sign(model.predict(X_test)) != Y_test.reshape(-1)) / len(X_test)\n",
    "    return E_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    num_runs = 1000\n",
    "    avg_E_in = 0.0 # The average E_in\n",
    "    avg_E_out = 0.0 # The average E_out\n",
    "    for r in range(num_runs):\n",
    "        # Generate target_w\n",
    "        target_w = generate_target_w()\n",
    "        \n",
    "        # Generate training set\n",
    "        X, Y = generate_data(NUM_TRAIN_POINTS, target_w)\n",
    "        \n",
    "        # Run Linear Regression to pick g and evaluate E_in\n",
    "        model, E_in = run_Linear_Regression_train(X, Y)\n",
    "\n",
    "        # Generate 1000 fresh points (test set)\n",
    "        X_test, Y_test = generate_data(NUM_TEST_POINTS, target_w)\n",
    "        \n",
    "        # Test g and evaluate E_out\n",
    "        E_out = run_Linear_Regression_test(X_test, Y_test, model)\n",
    "        \n",
    "        # Update average values of E_in and E_out\n",
    "        avg_E_in += (E_in * 1.0 / num_runs)\n",
    "        avg_E_out += (E_out * 1.0 / num_runs)\n",
    "    \n",
    "    # Print results\n",
    "    print('avg_E_in = %f' % (avg_E_in))\n",
    "    print('avg_E_out = %f' % (avg_E_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_E_in = 0.038900\n",
      "avg_E_out = 0.048054\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. \n",
    "\n",
    "avg_E_in is closest to the answer [c] 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. \n",
    "\n",
    "avg_E_out is closest to the answer [c] 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(w, x):\n",
    "    return np.sign(np.dot(x, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_converged(X, Y, w):\n",
    "    return np.array_equal(h(w, X), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PLA(X, Y, w):\n",
    "    \"\"\"\n",
    "    Runs PLA.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of g.\n",
    "    num_iterations : int\n",
    "        The number of iterations PLA takes to converge.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "\n",
    "    N = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        mis_X = []\n",
    "        mis_Y = []\n",
    "        iteration += 1\n",
    "        mix_id = np.random.permutation(N)\n",
    "        for i in range(N):\n",
    "            xi = X[mix_id[i], :]\n",
    "            yi = Y[mix_id[i], 0]\n",
    "            if h(w, xi)[0] != yi: # misclassified point\n",
    "                mis_X.append(xi)\n",
    "                mis_Y.append(yi)\n",
    "\n",
    "        # choose a point randomly from the set of misclassified points\n",
    "        if len(mis_X) > 0:\n",
    "            random_index = np.random.randint(0, len(mis_X))\n",
    "            x_i = mis_X[random_index]\n",
    "            y_i = mis_Y[random_index]\n",
    "            w = w + (y_i * x_i).reshape(-1, 1)\n",
    "        \n",
    "        if has_converged(X, Y, w):\n",
    "            break\n",
    "        \n",
    "    return w, iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(N):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of training examples.\n",
    "    \"\"\"\n",
    "    num_runs = 1000\n",
    "    avg_num_iterations = 0.0 # The average number of iterations PLA takes to converge\n",
    "    for r in range(num_runs):\n",
    "        # Generate target_w\n",
    "        target_w = generate_target_w()\n",
    "        \n",
    "        # Generate training set\n",
    "        X, Y = generate_data(N, target_w)\n",
    "\n",
    "        # Generate initial weights using Linear Regression\n",
    "        X_Dagger = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T)\n",
    "        w_init = np.dot(X_Dagger, Y)\n",
    "        \n",
    "        # Run PLA to pick g\n",
    "        _, num_iterations = run_PLA(X, Y, w_init)\n",
    "        \n",
    "        # Update average values\n",
    "        avg_num_iterations += (num_iterations * 1.0 / num_runs)\n",
    "    \n",
    "    # Print results\n",
    "    print('avg_num_iterations = %f' % (avg_num_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_num_iterations = 5.099000\n"
     ]
    }
   ],
   "source": [
    "main(N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_num_iterations is closest to the answer [a] 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "num_runs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genX():\n",
    "    X = np.random.uniform(-1, 1, size=(N, 2))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Y from X, based on the target function: $f(x_1, x_2) = sign(x_1^2 + x_2^2 - 0.6)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sign(x[0]**2 + x[1]**2 - 0.6).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_E_in = 0.504318\n"
     ]
    }
   ],
   "source": [
    "avg_E_in = 0.0 # The average E_in\n",
    "\n",
    "# Run experiment 1000 times\n",
    "for _ in range(num_runs):\n",
    "    # Generate X\n",
    "    X = genX()\n",
    "    Y = np.apply_along_axis(f, 1, X)\n",
    "\n",
    "    # Simulate noise by flipping the sign of the ouput in a randomly selected 10% subset\n",
    "    noisy_index = np.random.choice(N, int(N/10), replace=False)\n",
    "    for i in noisy_index:\n",
    "        Y[i] = np.negative(Y[i])\n",
    "\n",
    "    # with feature vector: (1, x_1, x_2)\n",
    "    X = np.insert(X, 0, 1, axis=1) # Add 'ones' column\n",
    "\n",
    "    # find the weight w\n",
    "    X_Dagger = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T)\n",
    "    w = np.dot(X_Dagger, Y) # the weight\n",
    "\n",
    "    # Make predictions from linear regression weights\n",
    "    Y_predict = np.sign(np.dot(X, w))\n",
    "\n",
    "    # E_in\n",
    "    E_in = sum(Y_predict != Y) / N\n",
    "\n",
    "    # take the average E_in to reduce variation in results\n",
    "    avg_E_in += (E_in * 1.0 / num_runs)\n",
    "\n",
    "print('avg_E_in = %f' % (avg_E_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_E_in is closest to the answer [d] 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_agree_a = 0.962332\n",
      "avg_agree_b = 0.663150\n",
      "avg_agree_c = 0.662927\n",
      "avg_agree_d = 0.632332\n",
      "avg_agree_e = 0.560737\n"
     ]
    }
   ],
   "source": [
    "avg_agree_a = 0.0 # The average probability of agreeing of [a]'s hypothesis\n",
    "avg_agree_b = 0.0 # The average probability of agreeing of [b]'s hypothesis\n",
    "avg_agree_c = 0.0 # The average probability of agreeing of [c]'s hypothesis\n",
    "avg_agree_d = 0.0 # The average probability of agreeing of [d]'s hypothesis\n",
    "avg_agree_e = 0.0 # The average probability of agreeing of [e]'s hypothesis\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    # Generate X\n",
    "    X = genX()\n",
    "\n",
    "    # Take x1s and x2s\n",
    "    x1s = X[:, 0]\n",
    "    x2s = X[:, 1]\n",
    "    Y = np.apply_along_axis(f, 1, X)\n",
    "\n",
    "    # Simulate noise by flipping the sign of the ouput in a randomly selected 10% subset\n",
    "    noisy_index = np.random.choice(N, int(N/10), replace=False)\n",
    "    for i in noisy_index:\n",
    "        Y[i] = np.negative(Y[i])\n",
    "\n",
    "    # Training data into the nonlinear feature vector: (1, x1, x2, x1x2, x1^2, x2^2)\n",
    "    X = np.array([np.ones(N), x1s, x2s, x1s * x2s, x1s**2, x2s**2]).T\n",
    "\n",
    "    # find the weight w\n",
    "    X_Dagger = np.dot(np.linalg.inv(np.dot(X.T, X)), X.T)\n",
    "    w = np.dot(X_Dagger, Y) # the weight\n",
    "\n",
    "    # Make predictions from linear regression weights\n",
    "    Y_predict = np.sign(np.dot(X, w))\n",
    "\n",
    "    # [a]\n",
    "    w_a = np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5])\n",
    "    Y_a = np.sign(np.dot(X, w_a))\n",
    "    agree_a = sum(Y_a == Y_predict) / N\n",
    "    avg_agree_a += (agree_a * 1.0 / num_runs)\n",
    "\n",
    "    # [b]\n",
    "    w_b = np.array([-1, -0.05, 0.08, 0.13, 1.5, 15])\n",
    "    Y_b = np.sign(np.dot(X, w_b))\n",
    "    agree_b = sum(Y_b == Y_predict) / N\n",
    "    avg_agree_b += (agree_b * 1.0 / num_runs)\n",
    "\n",
    "    # [c]\n",
    "    w_c = np.array([-1, -0.05, 0.08, 0.13, 15, 1.5])\n",
    "    Y_c = np.sign(np.dot(X, w_c))\n",
    "    agree_c = sum(Y_c == Y_predict) / N\n",
    "    avg_agree_c += (agree_c * 1.0 / num_runs)\n",
    "\n",
    "    # [d]\n",
    "    w_d = np.array([-1, -1.5, 0.08, 0.13, 0.05, 0.05])\n",
    "    Y_d = np.sign(np.dot(X, w_d))\n",
    "    agree_d = sum(Y_d == Y_predict) / N\n",
    "    avg_agree_d += (agree_d * 1.0 / num_runs)\n",
    "\n",
    "    # [e]\n",
    "    w_e = np.array([-1, -0.05, 0.08, 1.5, 0.15, 0.15])\n",
    "    Y_e = np.sign(np.dot(X, w_e))\n",
    "    agree_e = sum(Y_e == Y_predict) / N\n",
    "    avg_agree_e += (agree_e * 1.0 / num_runs)\n",
    "\n",
    "print('avg_agree_a = %f' % (avg_agree_a))\n",
    "print('avg_agree_b = %f' % (avg_agree_b))\n",
    "print('avg_agree_c = %f' % (avg_agree_c))\n",
    "print('avg_agree_d = %f' % (avg_agree_d))\n",
    "print('avg_agree_e = %f' % (avg_agree_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_agree_a is the highest value. That means hypothesis [a] has the highest probability of agreeing on a randomly selected point. Therefore, the answer is [a] $g(x_1, x_2) = sign(−1 − 0.05x_1 + 0.08x_2 + 0.13x_1x_2 + 1.5x_1^2 + 1.5x_2^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_E_out = 0.142855\n"
     ]
    }
   ],
   "source": [
    "avg_E_out = 0.0 # The average E_out\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    # Generate X\n",
    "    X = genX()\n",
    "    x1s = X[:, 0]\n",
    "    x2s = X[:, 1]\n",
    "    Y = np.apply_along_axis(f, 1, X)\n",
    "\n",
    "    # Simulate noise by flipping the sign of the ouput in a randomly selected 10% subset\n",
    "    noisy_index = np.random.choice(N, int(N/10), replace=False)\n",
    "    for i in noisy_index:\n",
    "        Y[i] = np.negative(Y[i])\n",
    "\n",
    "    # training data into the nonlinear feature vector\n",
    "    X = np.array([np.ones(N), x1s, x2s, x1s * x2s, x1s**2, x2s**2]).T\n",
    "\n",
    "    w = np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5]) # the weight of hypothesis from Problem 9\n",
    "\n",
    "    # Make predictions from linear regression weights\n",
    "    Y_predict = np.sign(np.dot(X, w))\n",
    "\n",
    "    # take the average E_in to reduce variation in results\n",
    "    E_out = sum(Y != Y_predict) / N\n",
    "    avg_E_out += (E_out * 1.0 / num_runs)\n",
    "\n",
    "print('avg_E_out = %f' % (avg_E_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_E_out is closest to the answer [b] 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "   [1] Github, Florian Peter, Caltech Machine Learning Homework # 2, last commit date: 30/04/2021, access date: 13/11/2023, https://github.com/workflow/caltech-machine-learning-homework/blob/master/HW2.ipynb\n",
    "\n",
    "   [2] notebook.community, Homework files, access date: 13/11/2023, https://notebook.community/akhileshh/lfd-caltech/homework02\n",
    "\n",
    "   [3] Github, Edgardo Deza, Homework 2, Problem 3 & Problem 4, last commit date: 11/10/2017, access date: 13/11/2023, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_2/homework_2_problem_3_4_Error_and_Noise.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
